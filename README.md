# ğŸ§  Máº¡ng NÆ¡-ron Tá»± XÃ¢y Dá»±ng: Nháº­n Diá»‡n Chá»¯ Viáº¿t Tay

Dá»± Ã¡n nÃ y triá»ƒn khai má»™t máº¡ng nÆ¡-ron truyá»n tháº³ng (**Feedforward Neural Network**) tá»« Ä‘áº§u báº±ng **NumPy**, dÃ¹ng Ä‘á»ƒ nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay (vÃ­ dá»¥: bá»™ dá»¯ liá»‡u MNIST).

![Kiáº¿n trÃºc mÃ´ hÃ¬nh](Images/2.png)

---

## ğŸ—ï¸ Kiáº¿n trÃºc mÃ´ hÃ¬nh

- **Táº§ng Ä‘áº§u vÃ o**: 784 nÆ¡-ron (áº£nh 28x28 Ä‘Æ°á»£c lÃ m pháº³ng)
- **Táº§ng áº©n**: `n_1` nÆ¡-ron (kÃ¨m hÃ m kÃ­ch hoáº¡t nhÆ° `ReLU`, `tanh`)
- **Táº§ng Ä‘áº§u ra**: 10 nÆ¡-ron (á»©ng vá»›i cÃ¡c chá»¯ sá»‘ tá»« 0 Ä‘áº¿n 9, dÃ¹ng softmax)

---

## âš™ï¸ CÃ¡c bÆ°á»›c thá»±c hiá»‡n

1. **Khá»Ÿi táº¡o tham sá»‘ ngáº«u nhiÃªn**
   - `W1`, `b1`, `W2`, `b2`

2. **Lan truyá»n xuÃ´i (Forward Propagation)**
   - \( Z_1 = W_1 \cdot X + b_1 \)
   - \( A_1 = f(Z_1) \)
   - \( Z_2 = W_2 \cdot A_1 + b_2 \)
   - \( A_2 = \text{Softmax}(Z_2) \)

3. **TÃ­nh hÃ m máº¥t mÃ¡t (Loss)**
   - DÃ¹ng cross-entropy:
     \[
     Loss = -\sum y_k \cdot \log(a_k)
     \]

4. **Lan truyá»n ngÆ°á»£c (Backward Propagation)**
   - TÃ­nh cÃ¡c Ä‘áº¡o hÃ m: `dZ2`, `dW2`, `dB2`, `dZ1`, `dW1`, `dB1`

5. **Cáº­p nháº­t tham sá»‘**
   - DÃ¹ng Gradient Descent:
     \[
     W = W - \alpha \cdot dW
     \]

---

## ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh

- TÃ­nh **Ä‘á»™ chÃ­nh xÃ¡c (accuracy)** trÃªn táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra.
- CÃ³ thá»ƒ thÃªm ma tráº­n nháº§m láº«n (confusion matrix) Ä‘á»ƒ trá»±c quan hoÃ¡.

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

